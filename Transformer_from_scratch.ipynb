{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f14d76-85a6-4570-835a-bf632d5ea236",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2100df-e9dc-4b3d-bcfd-7ea1222ad380",
   "metadata": {},
   "source": [
    "these characters have a integer associated with them defined by unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8544b096-dc43-4d6e-8aac-fc3cc686528a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2920a7-dc1c-42a4-b175-8b23c7f66fbc",
   "metadata": {},
   "source": [
    "These unicode numbers has a very large vocabulary, which we don't require for training and thus is not desirable for us.\n",
    "Thus we encode these into bytes (utf-8).\n",
    "Thus it restricts the mapping from 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e46989d3-151f-4557-96b8-9ed75490aebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(text.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebabbe5b-3df8-4160-91c1-3342647e349e",
   "metadata": {},
   "source": [
    "These are raw bytes which we will call tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e9cd217-fa74-467c-b1dc-0332c7b8f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = \"We used the Falcon chess engine as a baseline for our experiments. Falcon is a grandmaster-level chess program, which has successfully participated in several World Computer Chess Championships (WCCCs); in particular, it won second place at the World Computer Speed Chess Championship in 2008. Falconâ€™s extensiveevaluation function consists of more than 100 parameters, and its implementation contains several thousands of lines of code.  Despite all the computational improvements mentioned earlier for DeepChess, and numerous other implementation improvements which result in substantial additional computational speedup, DeepChess is still four times slower than Falconâ€™s own evaluation function. Nevertheless, we incorporate DeepChess into Falcon, completely replacing the evaluation function of the program. To measure the performance of DeepChess, we conducted a series of matches against Falcon, and also against the chess program Crafty. Crafty has successfully participated in numerous WCCCs, and is a direct descendant of Cray Blitz, the WCCC winner of 1983 and 1986. It has been frequently used in the literature as a standard reference. Each of the matches of DeepChess vs. Falcon and Crafty consisted of 100 games under a time control of 30 minutes per game for each side. Table 2 provides the results. As can be seen, DeepChess is on a par with Falcon. Falcon uses a manually tuned evaluation function developed over nearly ten years, containing more than a hundred parameters which grasp many subtle chess features. And yet, without any chess knowledge whatsoever (not even basic knowledge as the rules of chess), our DeepChess method managed to reach a level which is on a par with the manually tuned evaluation function of Falcon. The results also show that DeepChess is over 60 Elo [7] stronger than Crafty, a program which has won two WCCCs and has been manually tuned for thirty years. DeepChess performs on a par with Falcon despite the fact that it is four times slower. We ran a separate experiment where we allowed DeepChess to use four times more time than Falcon (2 hours vs 30 minutes). Running 100 such matches, DeepChess resoundingly defeated Falcon with a result of 63.5 - 36.5, corresponding to a 96 Elo performance difference. This shows that DeepChess is  actually not on par with Falconâ€™s evaluation function, but is considerably superior to it. In order to utilize the full potential of this enhanced chess understanding, it is critical to decrease the runtime of the neural network in the inference mode.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c31faf3c-cc57-4901-9e46-8fc534ac2e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 2540\n",
      "no. of tokens: 2546\n"
     ]
    }
   ],
   "source": [
    "tokens = train_text.encode(\"utf-8\") #raw bytes\n",
    "tokens = list(map(int, tokens)) #convert to a list of integers\n",
    "\n",
    "print(\"length\", len(train_text))\n",
    "print(\"no. of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e6af7d8-a2e8-445b-a81a-6b6f90b157fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {} #dictionary to store all the pairs\n",
    "    for pair in zip(ids, ids[1:]): \n",
    "        counts[pair] = counts.get(pair, 0) + 1  # pair: tuple[str, str]\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11cee294-a086-45ca-80de-8a4c56150c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734fcb21-627d-493d-9883-35d0da820d07",
   "metadata": {},
   "source": [
    "Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84a1619e-f50d-4c01-8248-880fd919119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b0543-02af-4465-9c27-569ed735b358",
   "metadata": {},
   "source": [
    "Now we need to find a sweet spot for the number of vocabulary terms vs number of tokens in our training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5057aa99-c965-401e-85fa-f3aeba1debfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (115, 32) into a new token 256\n",
      "merging (101, 32) into a new token 257\n",
      "merging (111, 110) into a new token 258\n",
      "merging (116, 104) into a new token 259\n",
      "merging (97, 108) into a new token 260\n",
      "merging (101, 115) into a new token 261\n",
      "merging (101, 114) into a new token 262\n",
      "merging (116, 105) into a new token 263\n",
      "merging (100, 32) into a new token 264\n",
      "merging (97, 110) into a new token 265\n",
      "merging (105, 110) into a new token 266\n",
      "merging (258, 32) into a new token 267\n",
      "merging (104, 261) into a new token 268\n",
      "merging (101, 110) into a new token 269\n",
      "merging (44, 32) into a new token 270\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 271 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b96a1797-f22e-4429-bfd0-63bac67889cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 2546\n",
      "ids length: 2022\n",
      "compression ratio: 1.26X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7481f665-d0d7-42f2-95b4-226f636cb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids, vocab):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e65459a6-485c-419a-811f-6c2ec6adbf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "  # given a string, return list of integers (the tokens)\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "  while len(tokens) >= 2:\n",
    "    stats = get_stats(tokens)\n",
    "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break # nothing else can be merged\n",
    "    idx = merges[pair]\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab8857-7f7b-4250-a92c-068f5c75d1d4",
   "metadata": {},
   "source": [
    "Now we have completed the training of the transformer.\n",
    "We will start embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0af7fd2b-7d98-442b-8786-9db52c87038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Embed():\n",
    "    def __init__(self, vocab_size, embed_dim, max_len):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maz_len = max_len\n",
    "\n",
    "        # Token Embedding: Maps token ids to a vector (learnable weights)\n",
    "        self.token_embed = np.random.randn(vocab_size, embed_dim) / np.sqrt(embed_dim)\n",
    "\n",
    "        #Position Embedding: Gives the token a position to get the position importance (Learnable weights in GPT-2, fixed in Attention is all you need paper)\n",
    "        self.pos_embed = np.random.randn(max_len, embed_dim) / np.sqrt(embed_dim)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        token_ids: (batch_size, seq_len), dtype: int\n",
    "        returns: (batch_size, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        # Lookup token embeddings\n",
    "        tok_embeds = self.token_embed[token_ids]  # (B, L, D)\n",
    "\n",
    "        # Broadcast position embeddings to batch\n",
    "        pos_embeds = self.pos_embed[np.arange(seq_len)]  # (L, D)\n",
    "        pos_embeds = np.broadcast_to(pos_embeds, (batch_size, seq_len, self.embed_dim))\n",
    "\n",
    "        return tok_embeds + pos_embeds  # final input embeddings\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b6e64-3799-4c2b-9411-89c8ba83ae0e",
   "metadata": {},
   "source": [
    "Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f0d53b5-f356-410e-83c9-1d33fe933d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiHeadSelfAttention:\n",
    "    def __init__(self, embed_dim=768, num_heads=12):\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = 1 / np.sqrt(self.head_dim)\n",
    "\n",
    "        # Q, K, V projection weights: [D, D]\n",
    "        self.W_q = np.random.randn(embed_dim, embed_dim) / np.sqrt(embed_dim)\n",
    "        self.W_k = np.random.randn(embed_dim, embed_dim) / np.sqrt(embed_dim)\n",
    "        self.W_v = np.random.randn(embed_dim, embed_dim) / np.sqrt(embed_dim)\n",
    "\n",
    "        # Output projection: [D, D]\n",
    "        self.W_o = np.random.randn(embed_dim, embed_dim) / np.sqrt(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L, D)\n",
    "        returns: (B, L, D)\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        H, d = self.num_heads, self.head_dim\n",
    "\n",
    "        # Linear projections\n",
    "        Q = x @ self.W_q   # (B, L, D)\n",
    "        K = x @ self.W_k\n",
    "        V = x @ self.W_v\n",
    "\n",
    "        # Reshape for heads: (B, H, L, d)\n",
    "        Q = Q.reshape(B, L, H, d).transpose(0, 2, 1, 3)\n",
    "        K = K.reshape(B, L, H, d).transpose(0, 2, 1, 3)\n",
    "        V = V.reshape(B, L, H, d).transpose(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled dot-product attention: (B, H, L, L)\n",
    "        attn_scores = Q @ K.transpose(0, 1, 3, 2) * self.scale\n",
    "\n",
    "        # Causal mask: prevent attending to future tokens\n",
    "        mask = np.triu(np.ones((L, L)), k=1).astype(bool)  # (L, L)\n",
    "        attn_scores[:, :, mask] = -1e10\n",
    "\n",
    "        # Softmax over last axis (L)\n",
    "        attn_weights = np.exp(attn_scores - np.max(attn_scores, axis=-1, keepdims=True))\n",
    "        attn_weights /= np.sum(attn_weights, axis=-1, keepdims=True)\n",
    "\n",
    "        # Weighted sum of V\n",
    "        attn_output = attn_weights @ V  # (B, H, L, d)\n",
    "\n",
    "        # Merge heads back: (B, L, D)\n",
    "        attn_output = attn_output.transpose(0, 2, 1, 3).reshape(B, L, D)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = attn_output @ self.W_o  # (B, L, D)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be683616-8aa2-4b6b-9a72-323d13f15259",
   "metadata": {},
   "source": [
    "Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f16bc08f-fab0-449d-a06f-8d1edb9ae76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, embed_dim=768, hidden_dim=3072):\n",
    "        self.W1 = np.random.randn(embed_dim, hidden_dim) / np.sqrt(embed_dim)\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "\n",
    "        self.W2 = np.random.randn(hidden_dim, embed_dim) / np.sqrt(hidden_dim)\n",
    "        self.b2 = np.zeros(embed_dim)\n",
    "\n",
    "    def gelu(self, x):\n",
    "        # Approximate GELU activation (used in GPT-2)\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L, D)\n",
    "        returns: (B, L, D)\n",
    "        \"\"\"\n",
    "        h = x @ self.W1 + self.b1  # (B, L, H)\n",
    "        h = self.gelu(h)           # GELU activation\n",
    "        out = h @ self.W2 + self.b2  # (B, L, D)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cc28235-df06-4620-a4ef-667560801009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputProjection:\n",
    "    def __init__(self, embedding_matrix):\n",
    "        self.embedding_matrix = embedding_matrix  # (V, D)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: (B, L, D)\n",
    "        return hidden_states @ self.embedding_matrix.T  # (B, L, V)\n",
    "\n",
    "    def predict(self, hidden_states):\n",
    "        logits = self.forward(hidden_states)  # (B, L, V)\n",
    "        return np.argmax(logits, axis=-1)     # (B, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0d3f5-2cc0-47d9-901b-d9b3da91ef23",
   "metadata": {},
   "source": [
    "Now lets test the model.. The outputs will be gibberish as the weights are randomly assigned and no training is done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db551da0-b7f2-40d2-95da-9eac00604f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: ï¿½ï¿½ï¿½bbbbbbbbbbbbb\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Engineering students\"\n",
    "input_ids = encode(input_text)  # list of ints\n",
    "\n",
    "# Convert to batch (1, L)\n",
    "input_ids = np.array([input_ids])  # shape: (1, L)\n",
    "\n",
    "vocab_size = 271\n",
    "embed_dim = 768\n",
    "max_len = 128\n",
    "\n",
    "embedder = Embed(vocab_size, embed_dim, max_len)\n",
    "attention = MultiHeadSelfAttention(embed_dim=embed_dim)\n",
    "ffn = FeedForward(embed_dim=embed_dim, hidden_dim=3072)\n",
    "output_proj = OutputProjection(embedder.token_embed)\n",
    "\n",
    "# Step 1: Embedding\n",
    "x = embedder.forward(input_ids)  # (1, L, D)\n",
    "\n",
    "# Step 2: Self-attention\n",
    "x = attention.forward(x)         # (1, L, D)\n",
    "\n",
    "# Step 3: Feed Forward\n",
    "x = ffn.forward(x)               # (1, L, D)\n",
    "\n",
    "# Step 4: Output projection\n",
    "logits = output_proj.forward(x)  # (1, L, V)\n",
    "\n",
    "# Step 5: Predict token ids\n",
    "pred_ids = np.argmax(logits, axis=-1)  # (1, L)\n",
    "\n",
    "# Convert output token ids back to text\n",
    "decoded_text = decode(pred_ids[0], vocab)\n",
    "print(\"Predicted text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341471c-c188-4730-820f-6a2bb4d70388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
